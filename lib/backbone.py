import keras.engine as KE
import keras.layers as KL
import keras.models as KM
import keras.backend as K
# import mrcnn.model as modellib
# import mrcnn.utils as utils
from .mrcnn.model import rpn_graph
from .mrcnn import model as modellib
from .mrcnn import utils as utils
import tensorflow as tf



############################################################
# Feature Pyramid Network & backbones
# Adapted from Siamese Mask R-CNN
############################################################

def build_resnet_model(config):
    input_image = KL.Input([None, None, 3], name='input_image')
    C1, C2, C3, C4, C5 = modellib.resnet_graph(input_image, config.BACKBONE,
                                               stage5=True, train_bn=config.TRAIN_BN)
    return KM.Model([input_image], [C1, C2, C3, C4, C5], name="resnet_model")


def build_fpn_model(feature_maps=128):
    # Define model to run resnet+fpn twice for image and target
    # Define input image
    C2 = KL.Input(shape=[None, None, 256], name="input_C2")
    C3 = KL.Input(shape=[None, None, 512], name="input_C3")
    C4 = KL.Input(shape=[None, None, 1024], name="input_C4")
    C5 = KL.Input(shape=[None, None, 2048], name="input_C5")
    # Compute fpn activations
    P2, P3, P4, P5, P6 = fpn_graph(C2, C3, C4, C5, feature_maps=feature_maps)
    # Return model
    return KM.Model([C2, C3, C4, C5], [P2, P3, P4, P5, P6], name="fpn_model")


def fpn_graph(C2, C3, C4, C5, feature_maps=128):
    P5 = KL.Conv2D(feature_maps, (1, 1), activation="relu", name='fpn_c5p5')(C5)
    P4 = KL.Add(name="fpn_p4add")([
        KL.UpSampling2D(size=(2, 2), name="fpn_p5upsampled")(P5),
        KL.Conv2D(feature_maps, (1, 1), activation="relu", name='fpn_c4p4')(C4)])
    P3 = KL.Add(name="fpn_p3add")([
        KL.UpSampling2D(size=(2, 2), name="fpn_p4upsampled")(P4),
        KL.Conv2D(feature_maps, (1, 1), activation="relu", name='fpn_c3p3')(C3)])
    P2 = KL.Add(name="fpn_p2add")([
        KL.UpSampling2D(size=(2, 2), name="fpn_p3upsampled")(P3),
        KL.Conv2D(feature_maps, (1, 1), activation="relu", name='fpn_c2p2')(C2)])
    # Attach 3x3 conv to all P layers to get the final feature maps.
    P2 = KL.Conv2D(feature_maps, (3, 3), activation="relu", padding="SAME", name="fpn_p2")(P2)
    P3 = KL.Conv2D(feature_maps, (3, 3), activation="relu", padding="SAME", name="fpn_p3")(P3)
    P4 = KL.Conv2D(feature_maps, (3, 3), activation="relu", padding="SAME", name="fpn_p4")(P4)
    P5 = KL.Conv2D(feature_maps, (3, 3), activation="relu", padding="SAME", name="fpn_p5")(P5)
    # P6 is used for the 5th anchor scale in RPN. Generated by
    # subsampling from P5 with stride of 2.
    P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name="fpn_p6")(P5)
    return [P2, P3, P4, P5, P6]
    
############################################################
# Detection aggregaters
############################################################
def custom_refine_detections_graph(rois, probs, deltas, window, supports, config):
    """ This is modified to output final detections with their concommitant supporting logits.
    :param supports: All supporting logits
    :return: detections and concommitant supporting logits
    Inputs:
        rois: [N, (y1, x1, y2, x2)] in normalized coordinates
        probs: [N, num_classes]. Class probabilities.
        deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Though we are not class-specific
        window: (y1, x1, y2, x2) in normalized coordinates. The part of the image
            that contains the image excluding the padding.
        supports: [N, sup_nb]. Support similarity probabilities.
    """
    # NOTE: no need to gather here, selecting which roi to pick
    # Class IDs per ROI
    class_ids = tf.argmax(probs, axis=1, output_type=tf.int32)
    # Class probability of the top class of each ROI
    indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)
    class_scores = tf.gather_nd(probs, indices)
    # Class-specific bounding box deltas
    deltas_specific = tf.gather_nd(deltas, indices)
    # Apply bounding box deltas
    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates
    refined_rois = modellib.apply_box_deltas_graph(
        rois, deltas_specific * config.BBOX_STD_DEV)
    # Clip boxes to image window
    refined_rois = modellib.clip_boxes_graph(refined_rois, window)

    # Filter out background boxes
    keep = tf.where(class_ids > 0)[:, 0]
    # Filter out low confidence boxes
    if config.DETECTION_MIN_CONFIDENCE:
        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]
        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),
                                        tf.expand_dims(conf_keep, 0))
        keep = tf.sparse_tensor_to_dense(keep)[0]

    # Apply per-class NMS
    # 1. Prepare variables
    pre_nms_class_ids = tf.gather(class_ids, keep)
    pre_nms_scores = tf.gather(class_scores, keep)
    pre_nms_rois = tf.gather(refined_rois, keep)
    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]

    def nms_keep_map(class_id):
        """Apply Non-Maximum Suppression on ROIs of the given class."""
        # Indices of ROIs of the given class
        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]
        # Apply NMS
        class_keep = tf.image.non_max_suppression(
            tf.gather(pre_nms_rois, ixs),
            tf.gather(pre_nms_scores, ixs),
            max_output_size=config.DETECTION_MAX_INSTANCES,
            iou_threshold=config.DETECTION_NMS_THRESHOLD)
        # Map indices
        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))
        # Pad with -1 so returned tensors have the same shape
        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]
        class_keep = tf.pad(class_keep, [(0, gap)],
                            mode='CONSTANT', constant_values=-1)
        # Set shape so map_fn() can infer result shape
        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])
        return class_keep

    # 2. Map over class IDs
    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,
                         dtype=tf.int64)
    # 3. Merge results into one list, and remove -1 padding
    nms_keep = tf.reshape(nms_keep, [-1])
    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])
    # 4. Compute intersection between keep and nms_keep
    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),
                                    tf.expand_dims(nms_keep, 0))
    keep = tf.sparse_tensor_to_dense(keep)[0]
    # Keep top detections
    roi_count = config.DETECTION_MAX_INSTANCES
    class_scores_keep = tf.gather(class_scores, keep)
    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)
    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]
    keep = tf.gather(keep, top_ids)

    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]
    # Coordinates are normalized.
    detections = tf.concat([
        tf.gather(refined_rois, keep),
        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],
        tf.gather(class_scores, keep)[..., tf.newaxis]
    ], axis=1)
    syn_supports = tf.gather(supports, keep)  # Change: gather at here

    # Pad with zeros if detections < DETECTION_MAX_INSTANCES
    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]
    detections = tf.pad(detections, [(0, gap), (0, 0)], "CONSTANT")
    syn_supports = tf.pad(syn_supports, [(0, gap), (0, 0)], "CONSTANT")
    return detections, syn_supports


class CustomDetectionLayer(KE.Layer):
    """ This layer is modified from the original detectionlayer of
    matterports' mask rcnn. It is hacked to filter out final detections over
    roi axis and support logits concurrently.
    """

    def __init__(self, config, **kwargs):
        super(CustomDetectionLayer, self).__init__(**kwargs)
        self.config = config

    def call(self, inputs):
        rois = inputs[0]
        mrcnn_class = inputs[1]
        mrcnn_bbox = inputs[2]
        image_meta = inputs[3]
        support_logits = inputs[4]
        m = modellib.parse_image_meta_graph(image_meta)
        image_shape = m['image_shape'][0]
        window = modellib.norm_boxes_graph(m['window'], image_shape[:2])
        detections_batch, syn_sup_logits = utils.batch_slice(
            [rois, mrcnn_class, mrcnn_bbox, window, support_logits],
            lambda v, w, x, y, z: custom_refine_detections_graph(v, w, x, y, z, self.config),
            self.config.IMAGES_PER_GPU)

        detections = tf.reshape(detections_batch,
                                [self.config.BATCH_SIZE, self.config.DETECTION_MAX_INSTANCES, 6])
        syn_sup_logits = tf.reshape(syn_sup_logits,
                                    [self.config.BATCH_SIZE, self.config.DETECTION_MAX_INSTANCES,
                                     self.config.SUPPORT_NUMBER])
        result = tf.concat([detections, syn_sup_logits], axis=-1)
        return result

    def compute_output_shape(self, input_shape):
        return (None, self.config.DETECTION_MAX_INSTANCES, self.config.SUPPORT_NUMBER + 6)


def negative_refine_detections_graph(rois, probs, deltas, window, config, nms=True):
    class_ids = tf.argmax(probs, axis=1, output_type=tf.int32)  # [N, num_classes] -> [N]
    indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)
    class_scores = tf.gather_nd(probs, indices)
    deltas_specific = tf.gather_nd(deltas, indices)
    refined_rois = modellib.apply_box_deltas_graph(
        rois, deltas_specific * config.BBOX_STD_DEV)
    refined_rois = modellib.clip_boxes_graph(refined_rois, window)

    neg_idx = tf.where(class_ids <= 0)[:, 0]  # use <= instead == to avoid error in tf.where()
    class_scores = tf.gather(class_scores, neg_idx)
    refined_rois = tf.gather(refined_rois, neg_idx)
    if nms:
        class_keep = tf.image.non_max_suppression(refined_rois, class_scores,
                                                  max_output_size=config.DETECTION_MAX_INSTANCES,
                                                  iou_threshold=config.DETECTION_NMS_THRESHOLD)
    else:
        class_keep = tf.random_shuffle(tf.range(class_scores.shape[0]))
        if config.DETECTION_MAX_INSTANCES > K.int_shape(class_keep)[0]:
            class_keep = tf.split(class_keep, [config.DETECTION_MAX_INSTANCES, -1], axis=0)[0]
    refined_rois = tf.gather(refined_rois, class_keep)
    class_scores = tf.gather(class_scores, class_keep)
    class_ids = tf.zeros_like(class_scores, dtype=tf.float32)
    detections = tf.concat([refined_rois, class_ids[..., tf.newaxis], class_scores[..., tf.newaxis]], axis=1)
    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]
    detections = tf.pad(detections, [(0, gap), (0, 0)], 'CONSTANT')
    return detections


class NegativeDetectionLayer(KE.Layer):
    """ This layer captures the negative rois output that are normally filtered out.
    This class is used in debug model mode.
    """
    def __init__(self, config, nms, **kwargs):
        super(NegativeDetectionLayer, self).__init__(**kwargs)
        self.config = config
        self.nms = nms

    def call(self, inputs):
        rois = inputs[0]
        mrcnn_class = inputs[1]
        mrcnn_bbox = inputs[2]
        image_meta = inputs[3]
        m = modellib.parse_image_meta_graph(image_meta)
        image_shape = m['image_shape'][0]
        window = modellib.norm_boxes_graph(m['window'], image_shape[:2])
        detections_batch = utils.batch_slice(
            [rois, mrcnn_class, mrcnn_bbox, window],
            lambda x, y, w, z: negative_refine_detections_graph(x, y, w, z, self.config, self.nms),
            self.config.IMAGES_PER_GPU)
        return tf.reshape(
            detections_batch,
            [self.config.BATCH_SIZE, self.config.DETECTION_MAX_INSTANCES, 6])

    def compute_output_shape(self, input_shape):
        return (None, self.config.DETECTION_MAX_INSTANCES, 6)
        
class ROIDetectionLayer(KE.Layer):
    """ This layer is used for clipping rois with normalized coordinates 
    of the reshaped input image.
    Returns [b, N, 5] where the last dim is 0 / 1 indicating whether it is a 
    padded detection.
    """
    def __init__(self, mode, config, **kwargs):
        super(ROIDetectionLayer, self).__init__(**kwargs)
        assert mode in ['proposal', 'detection_target']
        self.num_instances = config.POST_NMS_ROIS_INFERENCE if mode == 'proposal' else\
                             config.TRAIN_ROIS_PER_IMAGE 
        self.config = config
        
    def call(self, inputs):
        rois, image_meta = inputs
        if self.num_instances == self.config.TRAIN_ROIS_PER_IMAGE: 
            # If mode is detection_target, then we need to remove and add paddings.
            rois, _ = modellib.trim_zeros_graph(rois) 
        m = modellib.parse_image_meta_graph(image_meta)
        image_shape = m['image_shape'][0]
        window = modellib.norm_boxes_graph(m['window'], image_shape[:2])
        clipped_rois = utils.batch_slice([rois, window], 
                                          lambda x, y: modellib.clip_boxes_graph(x, y),
                                          self.config.IMAGES_PER_GPU)
        clipped_rois = tf.reshape(clipped_rois, [self.config.BATCH_SIZE, -1, 4])
        # Add the indicator.
        indicator = tf.reduce_max(tf.ones_like(clipped_rois, dtype=tf.float32), axis=-1, keepdims=True)
        clipped_rois = tf.concat([clipped_rois, indicator], axis=-1)
        # Add paddings for detection targets.
        if self.num_instances == self.config.TRAIN_ROIS_PER_IMAGE:
            P = tf.maximum(self.config.TRAIN_ROIS_PER_IMAGE - tf.shape(clipped_rois)[0], 0)
            clipped_rois = tf.pad(clipped_rois, [(0, 0), (0, P), (0, 0)])
        return clipped_rois
        
    def compute_output_shape(self, input_shape):
        return (None, self.num_instances, 5)

############################################################
# Custom Detection Target Layer
# Adapted from Matterport's Mask R-CNN
############################################################
def train_refine_detections_graph(rois, probs, deltas, window, config):
    rois, non_zeros = trim_zeros_graph(rois)
    class_ids = tf.argmax(probs, axis=1, output_type=tf.int32)
    class_ids = tf.boolean_mask(class_ids, non_zeros)
    
    indices = tf.boolean_mask(tf.range(probs.shape[0]), non_zeros)
    indices = tf.stack([indices, class_ids], axis=1)
    class_scores = tf.gather_nd(probs, indices)
    deltas_specific = tf.gather_nd(deltas, indices)

    refined_rois = modellib.apply_box_deltas_graph(rois, deltas_specific * config.BBOX_STD_DEV)
    refined_rois = modellib.clip_boxes_graph(refined_rois, window)

    detections = tf.concat([refined_rois, tf.to_float(class_ids[..., tf.newaxis]),
                            class_scores[..., tf.newaxis]], axis=1)
    gap = config.TRAIN_ROIS_PER_IMAGE - tf.shape(detections)[0]
    detections = tf.pad(detections, [(0, gap), (0, 0)], "CONSTANT")
    return detections

class TrainDetectionLayer(KE.Layer):
    def __init__(self, config, **kwargs):
        super(TrainDetectionLayer, self).__init__(**kwargs)
        self.config = config

    def call(self, inputs):
        """
        :param inputs: rois, mrcnn_class, mrcnn_bbox, image_meta
        :return: detection in [batch, rois, 6]
        """
        rois = inputs[0]
        mrcnn_class = inputs[1]
        mrcnn_bbox = inputs[2]
        image_meta = inputs[3]
        m = modellib.parse_image_meta_graph(image_meta)
        image_shape = m['image_shape'][0]
        window = modellib.norm_boxes_graph(m['window'], image_shape[:2])
        detections_batch = utils.batch_slice(
            [rois, mrcnn_class, mrcnn_bbox, window],
            lambda v, w, x, y: train_refine_detections_graph(v, w, x, y, self.config),
            self.config.IMAGES_PER_GPU)
        detections = tf.reshape(detections_batch,
                                [self.config.BATCH_SIZE, self.config.TRAIN_ROIS_PER_IMAGE, 6])
        return detections

    def compute_output_shape(self, input_shape):
        return (None, self.config.TRAIN_ROIS_PER_IMAGE, 6)

# Modify Detection Target Layer to filter out non-target rois.
def trim_zeros_graph(boxes, name='trim_zeros'):
    """ Overwrite original trim_zeros_graph to avoid an indigenous bug from
    Mask RCNN of matterport.
    """
    non_zeros = tf.cast(tf.maximum(tf.reduce_sum(boxes, axis=1), 0.0), tf.bool)
    boxes = tf.boolean_mask(boxes, non_zeros, name=name)
    return boxes, non_zeros

def detection_targets_graph(proposals, gt_class_ids, gt_boxes, gt_masks, config):
    """Generates detection targets for one image. Subsamples proposals and
    generates target class IDs, bounding box deltas, and masks for each.
    Inputs:
    proposals: [POST_NMS_ROIS_TRAINING, (y1, x1, y2, x2)] in normalized coordinates. Might
               be zero padded if there are not enough proposals.
    gt_class_ids: [MAX_GT_INSTANCES] int class IDs
    gt_boxes: [MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized coordinates.
    gt_masks: [height, width, MAX_GT_INSTANCES] of boolean type.
    Returns: Target ROIs and corresponding class IDs, bounding box shifts,
    and masks.
    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates
    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs. Zero padded.
    deltas: [TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw))]
    masks: [TRAIN_ROIS_PER_IMAGE, height, width]. Masks cropped to bbox
           boundaries and resized to neural network output size.
    Note: Returned arrays might be zero padded if not enough target ROIs.
    """
    # Assertions
    asserts = [
        tf.Assert(tf.greater(tf.shape(proposals)[0], 0), [proposals],
                  name="roi_assertion"),
    ]
    with tf.control_dependencies(asserts):
        proposals = tf.identity(proposals)  # proposals - [500, 4]
    # Remove zero padding
    # Change: Mask RCNN has bug here, use our trim_zeros_graph instead.
    proposals, _ = modellib.trim_zeros_graph(proposals, name="trim_proposals") 
    gt_boxes, non_zeros = trim_zeros_graph(gt_boxes, name="trim_gt_boxes")
    gt_class_ids = tf.boolean_mask(gt_class_ids, non_zeros,
                                   name="trim_gt_class_ids")
    gt_masks = tf.gather(gt_masks, tf.where(non_zeros)[:, 0], axis=2,
                         name="trim_gt_masks")

    # Compute overlaps matrix [proposals, gt_boxes]
    overlaps = modellib.overlaps_graph(proposals, gt_boxes)
    # overlaps - [(num_proposals), (num_gt_boxes)]
    # CHANGE: compute sizes of each proposal and set iou threshold adaptively
    y1, x1, y2, x2 = tf.split(proposals, 4, axis=1)  # [500, 1]
    size = tf.squeeze((y2 - y1) * (x2 - x1), axis=-1)  # [500, 1]
    # CHANGE: pick the max proposals for each ground truth
    gt_iou_argmax = K.argmax(overlaps, axis=0) 

    # Determine positive and negative ROIs
    roi_iou_max = tf.reduce_max(overlaps, axis=1) 

    positive_roi_bool = (roi_iou_max >= 0.5)

    positive_indices = tf.where(positive_roi_bool)[:, 0]
    negative_roi_bool = tf.logical_not(positive_roi_bool)
    negative_indices = tf.where(negative_roi_bool)[:, 0]

    # Change: Subsample ROIs if positive rois are more than TRAIN_ROIS
    positive_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)
    positive_indices = tf.random_shuffle(positive_indices)[:positive_count]
    positive_count = tf.shape(positive_indices)[0] 
    
    positive_rois = tf.gather(proposals, positive_indices)
    r = 1.0 / config.ROI_POSITIVE_RATIO
    negative_count = tf.maximum(tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count, 1)
    negative_indices = tf.random_shuffle(negative_indices)[:negative_count]
    negative_rois = tf.gather(proposals, negative_indices)
    # positive_rois - [positive_count, 4]

    # Assign positive ROIs to GT boxes.
    positive_overlaps = tf.gather(overlaps, positive_indices)
    # positive_rois - [positive_count, 30]

    roi_gt_box_assignment = tf.cond(
        tf.greater(tf.shape(positive_overlaps)[1], 0),
        true_fn=lambda: tf.argmax(positive_overlaps, axis=1),  # maybe "tf.shape(positive_overlaps)[0]"
        false_fn=lambda: tf.cast(tf.constant([]), tf.int64)
    )  # if true return the bigest iou of GT for the positive roi, else none

    roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)
    roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)

    # Compute bbox refinement for positive ROIs
    deltas = utils.box_refinement_graph(positive_rois, roi_gt_boxes)
    deltas /= config.BBOX_STD_DEV

    # Assign positive ROIs to GT masks
    # Permute masks to [N, height, width, 1]
    transposed_masks = tf.expand_dims(tf.transpose(gt_masks, [2, 0, 1]), -1)
    # Pick the right mask for each ROI
    roi_masks = tf.gather(transposed_masks, roi_gt_box_assignment)

    # Compute mask targets
    boxes = positive_rois
    if config.USE_MINI_MASK:
        # Transform ROI coordinates from normalized image space
        # to normalized mini-mask space.
        y1, x1, y2, x2 = tf.split(positive_rois, 4, axis=1)
        gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(roi_gt_boxes, 4, axis=1)
        gt_h = gt_y2 - gt_y1
        gt_w = gt_x2 - gt_x1
        y1 = (y1 - gt_y1) / gt_h
        x1 = (x1 - gt_x1) / gt_w
        y2 = (y2 - gt_y1) / gt_h
        x2 = (x2 - gt_x1) / gt_w
        boxes = tf.concat([y1, x1, y2, x2], 1)
    box_ids = tf.range(0, tf.shape(roi_masks)[0])
    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes,
                                     box_ids,
                                     config.MASK_SHAPE)
    # Remove the extra dimension from masks.
    masks = tf.squeeze(masks, axis=3)

    # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with
    # binary cross entropy loss.
    masks = tf.round(masks)

    # Append negative ROIs and pad bbox deltas and masks that
    # are not used for negative ROIs with zeros.
    rois = tf.concat([positive_rois, negative_rois], axis=0)
    N = tf.shape(negative_rois)[0]
    P = tf.maximum(config.TRAIN_ROIS_PER_IMAGE - tf.shape(rois)[0], 0)
    rois = tf.pad(rois, [(0, P), (0, 0)])
    roi_gt_boxes = tf.pad(roi_gt_boxes, [(0, P + N), (0, 0)])
    roi_gt_class_ids = tf.pad(roi_gt_class_ids, [(0, P + N)])
    deltas = tf.pad(deltas, [(0, P + N), (0, 0)])
    masks = tf.pad(masks, [[0, P + N], (0, 0), (0, 0)])
    padding = tf.reshape(config.TRAIN_ROIS_PER_IMAGE - (P + N), (1,))
    return rois, roi_gt_class_ids, deltas, masks, padding


class DetectionTargetLayer(KE.Layer):
    """Subsamples proposals and generates target box refinement, class_ids,
    and masks for each.
    Inputs:
    proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might
               be zero padded if there are not enough proposals.
    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.
    gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized
              coordinates.
    gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type
    Returns: Target ROIs and corresponding class IDs, bounding box shifts,
    and masks.
    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized
          coordinates
    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.
    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw)]
    target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width]
                 Masks cropped to bbox boundaries and resized to neural
                 network output size.
    paddings: [batch, 1] Number of paddings on roi dimension of each batch.
    Note: Returned arrays might be zero padded if not enough target ROIs.

    rois, target_class_ids, target_bbox, target_mask, paddings = 
    DetectionTargetLayer(config, name="proposal_targets")([
        target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])
    """

    def __init__(self, config, **kwargs):
        super(DetectionTargetLayer, self).__init__(**kwargs)
        self.config = config

    def call(self, inputs):
        proposals = inputs[0]
        gt_class_ids = inputs[1]
        gt_boxes = inputs[2]
        gt_masks = inputs[3]

        # Slice the batch and run a graph for each slice
        names = ["rois", "target_class_ids", "target_deltas", "target_mask", "paddings"]
        outputs = utils.batch_slice(
            [proposals, gt_class_ids, gt_boxes, gt_masks],
            lambda w, x, y, z: detection_targets_graph(
                w, x, y, z, self.config),
            self.config.IMAGES_PER_GPU, names=names)
        return outputs

    def compute_output_shape(self, input_shape):
        return [
            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # rois
            (None, self.config.TRAIN_ROIS_PER_IMAGE),  # class_ids
            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # deltas
            (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.MASK_SHAPE[0],
             self.config.MASK_SHAPE[1]),  # masks
            (None, 1)  # paddings
        ]

    def compute_mask(self, inputs, mask=None):
        return [None, None, None, None, None]




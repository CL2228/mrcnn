import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)
sess_config = tf.ConfigProto()

import sys
import os

COCO_DATA = '/mnt/Disk4/zbfan/coco/'
MASK_RCNN_MODEL_PATH = 'lib/Mask_RCNN/'
ROOT_DIR = os.getcwd()

if MASK_RCNN_MODEL_PATH not in sys.path:
    sys.path.append(MASK_RCNN_MODEL_PATH)

from samples.coco import coco
# from mrcnn import utils
# from mrcnn import model as modellib
# from mrcnn import visualize
from lib.mrcnn import utils
from lib.mrcnn import model as modellib
from lib.mrcnn import visualize

from lib import utils as m_utils
from lib import model as m_model
from lib import config as m_config

import time
import datetime
import random
import numpy as np
import skimage.io
import imgaug
import pickle
import matplotlib.pyplot as plt
from collections import OrderedDict
import keras as K

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

### Eval Function ###
def evaluate_dataset(model, dataset, dataset_object, dataset_type='coco',
                     limit=-1, class_index=None, verbose=1):
    assert dataset_type in ['coco'] and model.config.WAYS == 1
    if limit < 0:
        limit = len(dataset.image_ids)
    limit = max(limit, len(dataset.image_ids))
    image_ids = dataset.image_ids[:limit]
    dataset_image_ids = [dataset.image_info[id]["id"] for id in image_ids]
    generator = m_utils.stage1_validation_generator(dataset, model.config, shuffle=False)
    t_prediction = 0
    t_start = time.time()
    results = []
    for i in range(limit):
        if i % 100 == 0 and verbose > 1:
            print("Processing image {}/{} ...".format(i, len(image_ids)))
        model_inp, _, target_class_ids = next(generator)
        meta = model_inp[1]
        m = modellib.parse_image_meta(meta)
        image_id = m['image_id'][0:1]
        r = model.pseudo_detect(model_inp)
        r['class_ids'] = np.array(target_class_ids[0] for j in r['class_ids'])
        t_prediction += (time.time() - t)
        if dataset_type == 'coco':
            image_results = coco.build_coco_results(dataset, image_id, r['rois'], r['class_ids'], 
                                                    r['scores'], r['masks'].astype(np.uint8))
        results.extend(image_results)
    dataset_results = dataset_object.loadRes(results)
    eval_type = 'bbox'
    cocoEval = m_utils.customCOCOeval(dataset_object, dataset_results, eval_type)
    cocoEval.params.imgIds = dataset_image_ids
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize(class_index=class_index, verbose=verbose)
    if verbose > 0:
        print("Prediction time: {}. Average {}/image".format(
                t_prediction, t_prediction / len(image_ids)))
        print("Total time: ", time.time() - t_start)
            


### Main ###
from optparse import OptionParser

class EvalConfig(m_config.Config):
    # Set batch size to 1 since we'll be running inference on
    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    NAME = 'coco'
    EXPERIMENT = 'spatial_lr=e-3'
    CHECKPOINT_DIR = 'checkpoints/'
    # Adapt loss weights
    LOSS_WEIGHTS = {'rpn_class_loss': 2.0,
                    'rpn_bbox_loss': 1.0}
    DETECTION_NMS_THRESHOLD = 0.5
    POST_NMS_ROIS_INFERENCE = 500
    RPN_NMS_THRESHOLD = 0.7
    ATTENTION = 'spatial'

config = EvalConfig()

train_schedule = OrderedDict()
train_schedule[200] = {"learning_rate": config.LEARNING_RATE, "layers": "4+"}

parser = OptionParser('')
parser.add_option('--model', dest='model', help='path to weight of the model')
parser.add_option('--dataset', dest='dataset', help='val / train / all is supported')
parser.add_option('--classes', dest='classes', help='which classes to evaluate')
parser.add_option('--ways', dest='ways', help='number of ways to enable')
parser.add_option('--shots', dest='shots', help='number of shots to offer')
parser.add_option('--limit', dest='limit', help='number of images to evaluate in the dataset')
parser.add_option('--verbose', dest='verbose')
parser.add_option('--nms', dest='nms', help='nms threshold')
parser.add_option('--rois', dest='rois', help='number of proposals generated by rpn')
options, args = parser.parse_args()

verbose = 1 if options.verbose is None else int(options.verbose)
limit = -1 if options.limit is None else int(options.limit)
shots = 1 if options.shots is None else int(options.shots)
ways = 1 if options.ways is None else int(options.ways)
classes_type = options.classes or 'val'
dataset_type = options.classes or 'val'

# Manually set is supported.
path = options.model or 'convert_weight/matching_repmet_rpn_lr=e-3_0200.h5' 

assert dataset_type in ['train', 'val', 'all']
assert classes_type in ['train', 'val', 'all']

config.USE_RPN_ROIS_INFERENCE = not randomroi
config.POST_NMS_ROIS_INFERENCE = float(options.rois)
config.WAYS = ways
config.SHOTS = shots
config.SUPPORT_NUMBER = config.WAYS * config.SHOTS
if options.nms is not None:
    config.RPN_NMS_THRESHOLD = float(options.nms)

MODEL_DIR = os.path.join(ROOT_DIR, "logs_stage1")
model = m_model.Stage1Network(mode="inference", model_dir=MODEL_DIR, config=config)
model.load_checkpoint(path, training_schedule=train_schedule)

train_classes = []
eval_classes = []
for i in range(1, 81):
    if i % 5 != 0:
        train_classes.append(i)
    else:
        eval_classes.append(i)
train_classes = np.array(train_classes)
eval_classes = np.array(eval_classes)

datasets = []
if dataset_type == 'train' or dataset_type == 'all':
    coco_train = m_utils.IndexedCocoDataset()
    coco_train_obj = coco_train.load_coco(COCO_DATA, "train", year="2017", return_coco=True)
    coco_train.prepare()
    coco_train.build_indices()
    datasets.append([coco_train, coco_train_obj])
if dataset_type == 'val' or dataset_type == 'all':
    coco_eval = m_utils.IndexedCocoDataset()
    coco_eval_obj = coco_eval.load_coco(COCO_DATA, "val", year="2017", return_coco=True)
    coco_eval.prepare()
    coco_eval.build_indices()  
    datasets.append([coco_eval, coco_eval_obj])
    
classes = []
if classes_type == 'train' or classes_type == 'all':
    classes.append(train_classes)
if classes_type == 'val' or classes_type == 'all':
    classes.append(eval_classes)

random_rois = config.POST_NMS_ROIS_INFERENCE if not config.USE_RPN_ROIS_INFERENCE else 0
for d, d_obj in datasets:
    for c in classes:
        d.ACTIVE_CLASSES = c
        evaluate_dataset(model, d, d_obj, limit=limit, verbose=verbose)
